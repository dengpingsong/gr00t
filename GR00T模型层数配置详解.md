# GR00T æ¨¡å‹æ¶æ„ä¸å±‚æ•°é…ç½®è¯¦è§£

## æ¦‚è§ˆ

GR00T N1.5-3B æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æœºå™¨äººç­–ç•¥æ¨¡å‹ï¼Œé‡‡ç”¨äº†**åŒè„‘æ¶æ„**ï¼ˆDual Brainï¼‰ï¼ŒåŒ…å«è§†è§‰-è¯­è¨€éª¨å¹²ç½‘ç»œï¼ˆBackboneï¼‰å’ŒåŠ¨ä½œå¤´ï¼ˆAction Headï¼‰ã€‚ä¸‹é¢è¯¦ç»†åˆ†æå„ç»„ä»¶çš„å±‚æ•°è®¾ç½®å’Œå‚æ•°é…ç½®ã€‚

## ğŸ§  æ¨¡å‹æ€»ä½“æ¶æ„

```
GR00T N1.5-3B æ¨¡å‹æ¶æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Backbone (EAGLE)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Vision Model â”‚  â”‚ Language Modelâ”‚ â”‚ 
â”‚  â”‚   27 layers  â”‚  â”‚   12 layers   â”‚ â”‚
â”‚  â”‚   (SigLIP)   â”‚  â”‚   (Qwen3)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Action Head (Flow Matching)  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚VL Self-Attn â”‚  â”‚ DiT Diffusion â”‚ â”‚
â”‚  â”‚   4 layers  â”‚  â”‚   16 layers   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ 1. Backbone é…ç½® (EAGLE)

### 1.1 Language Model (Qwen3-1.7B)

```json
"text_config": {
    "architectures": ["Qwen3ForCausalLM"],
    "hidden_size": 2048,                    // éšè—å±‚ç»´åº¦
    "num_hidden_layers": 28,                // æ€»å±‚æ•°: 28å±‚
    "num_attention_heads": 16,              // æ³¨æ„åŠ›å¤´æ•°: 16ä¸ª
    "num_key_value_heads": 8,               // KVå¤´æ•°: 8ä¸ª (GQA)
    "intermediate_size": 6144,              // FFNä¸­é—´å±‚ç»´åº¦
    "head_dim": 128,                        // æ¯ä¸ªæ³¨æ„åŠ›å¤´ç»´åº¦
    "max_position_embeddings": 40960,       // æœ€å¤§ä½ç½®ç¼–ç 
    "attention_dropout": 0,                 // æ³¨æ„åŠ›dropout
    "hidden_act": "silu",                   // æ¿€æ´»å‡½æ•°
    "rms_norm_eps": 1e-6,                   // RMSå½’ä¸€åŒ–å‚æ•°
}
```

**ä½†æ˜¯åœ¨GR00Tä¸­å®é™…ä½¿ç”¨ï¼š**
```python
# åœ¨ eagle_backbone.py ä¸­
select_layer: int = 12    # åªä½¿ç”¨å‰12å±‚!!!

# é€šè¿‡ä»¥ä¸‹ä»£ç å®ç°å±‚æ•°è£å‰ª
while len(self.eagle_model.language_model.model.layers) > select_layer:
    self.eagle_model.language_model.model.layers.pop(-1)
```

**âœ¨ å…³é”®ç‚¹**: è™½ç„¶Qwen3æœ‰28å±‚ï¼Œä½†GR00Tåªä½¿ç”¨å‰12å±‚æ¥èŠ‚çœè®¡ç®—èµ„æºï¼

### 1.2 Vision Model (SigLIP)

```json
"vision_config": {
    "model_type": "siglip_vision_model",
    "hidden_size": 1152,                    // éšè—å±‚ç»´åº¦
    "num_hidden_layers": 27,                // è§†è§‰ç¼–ç å™¨å±‚æ•°: 27å±‚
    "num_attention_heads": 16,              // æ³¨æ„åŠ›å¤´æ•°: 16ä¸ª
    "intermediate_size": 4304,              // FFNä¸­é—´å±‚ç»´åº¦
    "image_size": 224,                      // è¾“å…¥å›¾åƒå°ºå¯¸
    "patch_size": 14,                       // patchå¤§å°
    "num_channels": 3,                      // è¾“å…¥é€šé“æ•°
    "attention_dropout": 0,                 // æ³¨æ„åŠ›dropout
    "hidden_act": "gelu_pytorch_tanh",      // æ¿€æ´»å‡½æ•°
}
```

### 1.3 Backbone è®­ç»ƒé…ç½®

```python
# åœ¨ so101-checkpoints/config.json ä¸­
"backbone_cfg": {
    "select_layer": 12,          # LLMä½¿ç”¨å‰12å±‚
    "tune_llm": false,          # ä¸å¾®è°ƒè¯­è¨€æ¨¡å‹
    "tune_visual": true,        # å¾®è°ƒè§†è§‰æ¨¡å‹
    "project_to_dim": null,     # æŠ•å½±ç»´åº¦ï¼ˆä½¿ç”¨é»˜è®¤1536ï¼‰
}
```

## ğŸš€ 2. Action Head é…ç½®

### 2.1 VL Self-Attention æ¨¡å—

```json
"vl_self_attention_cfg": {
    "num_layers": 4,                        // è‡ªæ³¨æ„åŠ›å±‚æ•°: 4å±‚
    "num_attention_heads": 32,              // æ³¨æ„åŠ›å¤´æ•°: 32ä¸ª
    "attention_head_dim": 64,               // æ¯ä¸ªå¤´ç»´åº¦: 64
    "dropout": 0.2,                         // dropoutç‡
    "final_dropout": true,                  // æœ€åå±‚dropout
    "positional_embeddings": null,          // ä½ç½®ç¼–ç 
}
```

### 2.2 DiT (Diffusion Transformer) æ¨¡å—

```json
"diffusion_model_cfg": {
    "num_layers": 16,                       // DiTå±‚æ•°: 16å±‚ â­
    "num_attention_heads": 32,              // æ³¨æ„åŠ›å¤´æ•°: 32ä¸ª
    "attention_head_dim": 48,               // æ¯ä¸ªå¤´ç»´åº¦: 48
    "output_dim": 1024,                     // è¾“å‡ºç»´åº¦
    "cross_attention_dim": 2048,            // äº¤å‰æ³¨æ„åŠ›ç»´åº¦
    "dropout": 0.2,                         // dropoutç‡
    "final_dropout": true,                  // æœ€åå±‚dropout
    "interleave_self_attention": true,      // äº¤é”™è‡ªæ³¨æ„åŠ›
    "norm_type": "ada_norm",                // å½’ä¸€åŒ–ç±»å‹
    "positional_embeddings": null,          // ä½ç½®ç¼–ç 
}
```

### 2.3 Action Head å…¶ä»–é…ç½®

```json
"action_head_cfg": {
    "action_dim": 32,                       // åŠ¨ä½œç»´åº¦
    "action_horizon": 16,                   // åŠ¨ä½œé¢„æµ‹æ­¥é•¿
    "hidden_size": 1024,                    // éšè—å±‚ç»´åº¦
    "input_embedding_dim": 1536,            // è¾“å…¥åµŒå…¥ç»´åº¦
    "backbone_embedding_dim": 2048,         // éª¨å¹²ç½‘ç»œåµŒå…¥ç»´åº¦
    "max_action_dim": 32,                   // æœ€å¤§åŠ¨ä½œç»´åº¦
    "max_state_dim": 64,                    // æœ€å¤§çŠ¶æ€ç»´åº¦
    "num_inference_timesteps": 4,           // æ¨ç†æ—¶é—´æ­¥æ•°
    "num_timestep_buckets": 1000,           // æ—¶é—´æ­¥æ¡¶æ•°
    "tune_diffusion_model": true,           // å¾®è°ƒæ‰©æ•£æ¨¡å‹
    "tune_projector": true,                 // å¾®è°ƒæŠ•å½±å™¨
}
```

## ğŸ“Š 3. å‚æ•°ç»Ÿè®¡

### 3.1 å„ç»„ä»¶å‚æ•°é‡

æ ¹æ®ä»£ç ä¸­çš„æ‰“å°ä¿¡æ¯ï¼š

```python
# DiT æ¨¡å—å‚æ•°é‡
print("Total number of DiT parameters: ", 
      sum(p.numel() for p in self.parameters() if p.requires_grad))
# è¾“å‡º: Total number of DiT parameters: 550386688  (çº¦5.5äº¿å‚æ•°)

# VL Self-Attention æ¨¡å—å‚æ•°é‡  
print("Total number of SelfAttentionTransformer parameters: ", 
      sum(p.numel() for p in self.parameters() if p.requires_grad))
# è¾“å‡º: Total number of SelfAttentionTransformer parameters: 201433088  (çº¦2äº¿å‚æ•°)
```

### 3.2 å±‚æ•°æ±‡æ€»è¡¨

| ç»„ä»¶ | å­æ¨¡å— | å±‚æ•° | æ³¨æ„åŠ›å¤´æ•° | å¤´ç»´åº¦ | éšè—ç»´åº¦ | æ˜¯å¦å¾®è°ƒ |
|------|--------|------|------------|--------|----------|----------|
| **Backbone** | Vision (SigLIP) | 27 | 16 | 72 | 1152 | âœ… |
| | Language (Qwen3) | 12* | 16 | 128 | 2048 | âŒ |
| **Action Head** | VL Self-Attention | 4 | 32 | 64 | 2048 | âœ… |
| | DiT Diffusion | 16 | 32 | 48 | 1536 | âœ… |

*æ³¨ï¼šè¯­è¨€æ¨¡å‹è™½ç„¶åŸæœ¬æœ‰28å±‚ï¼Œä½†å®é™…åªä½¿ç”¨å‰12å±‚

## ğŸ”§ 4. å±‚æ•°è®¾è®¡åŸç†

### 4.1 Backbone å±‚æ•°é€‰æ‹©

```python
# Language Model: 12å±‚ (è€ŒéåŸå§‹28å±‚)
# åŸå› ï¼š
# 1. æœºå™¨äººä»»åŠ¡ä¸éœ€è¦å¤æ‚çš„è¯­è¨€ç†è§£
# 2. èŠ‚çœè®¡ç®—èµ„æºå’Œæ¨ç†é€Ÿåº¦
# 3. é¿å…è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

# Vision Model: 27å±‚ (ä¿æŒå®Œæ•´)
# åŸå› ï¼š
# 1. è§†è§‰ç‰¹å¾æå–å¯¹æœºå™¨äººä»»åŠ¡è‡³å…³é‡è¦
# 2. éœ€è¦ä¸°å¯Œçš„è§†è§‰è¡¨ç¤ºæ¥ç†è§£å¤æ‚åœºæ™¯
# 3. æ”¯æŒå¤šè§†è§’èåˆå’Œç»†èŠ‚æ•æ‰
```

### 4.2 Action Head å±‚æ•°é€‰æ‹©

```python
# VL Self-Attention: 4å±‚
# åŸå› ï¼š
# 1. è¶³å¤Ÿèåˆè§†è§‰å’Œè¯­è¨€ç‰¹å¾
# 2. ä¸ä¼šè¿‡åº¦å¤æ‚åŒ–ç‰¹å¾è¡¨ç¤º
# 3. ä¿æŒè®¡ç®—æ•ˆç‡

# DiT Diffusion: 16å±‚  
# åŸå› ï¼š
# 1. æ‰©æ•£æ¨¡å‹éœ€è¦è¶³å¤Ÿæ·±åº¦æ¥å­¦ä¹ åŠ¨ä½œåˆ†å¸ƒ
# 2. Flow Matching éœ€è¦å¤æ‚çš„å™ªå£°-åŠ¨ä½œæ˜ å°„
# 3. æ”¯æŒå¤šæ­¥åŠ¨ä½œåºåˆ—ç”Ÿæˆ
```

## âš™ï¸ 5. è®­ç»ƒç­–ç•¥

### 5.1 åˆ†ç»„è®­ç»ƒé…ç½®

```python
# å¾®è°ƒé…ç½®
tune_llm = False           # å†»ç»“è¯­è¨€æ¨¡å‹ï¼ˆèŠ‚çœGPUå†…å­˜ï¼‰
tune_visual = True         # å¾®è°ƒè§†è§‰æ¨¡å‹ï¼ˆå­¦ä¹ æ–°è§†è§‰ç‰¹å¾ï¼‰
tune_projector = True      # å¾®è°ƒæŠ•å½±å™¨ï¼ˆé€‚é…æ–°ä»»åŠ¡ï¼‰
tune_diffusion_model = True # å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼ˆå­¦ä¹ æ–°åŠ¨ä½œåˆ†å¸ƒï¼‰
```

### 5.2 è®¡ç®—ç±»å‹é…ç½®

```python
compute_dtype = "bfloat16"  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
model_dtype = "float32"     # æ¨¡å‹æƒé‡ç²¾åº¦
```

## ğŸ¯ 6. æ¨ç†é…ç½®

### 6.1 åŠ¨ä½œç”Ÿæˆé…ç½®

```python
action_horizon = 16         # é¢„æµ‹æœªæ¥16æ­¥åŠ¨ä½œ
num_inference_timesteps = 4 # æ‰©æ•£é‡‡æ ·4æ­¥
noise_s = 0.999            # å™ªå£°è°ƒåº¦å‚æ•°
```

### 6.2 å¤šå®ä½“æ”¯æŒ

```python
max_num_embodiments = 32    # æ”¯æŒæœ€å¤š32ç§æœºå™¨äººå®ä½“
# é€šè¿‡ CategorySpecificLinear ä¸ºæ¯ç§æœºå™¨äººå­¦ä¹ ä¸“é—¨çš„å‚æ•°
```

## ğŸ” 7. å…³é”®è®¾è®¡äº®ç‚¹

1. **åˆ†å±‚å¾®è°ƒ**ï¼šåªå¾®è°ƒéœ€è¦çš„ç»„ä»¶ï¼ŒèŠ‚çœè®¡ç®—èµ„æº
2. **å±‚æ•°ä¼˜åŒ–**ï¼šè¯­è¨€æ¨¡å‹å‰ªæåˆ°12å±‚ï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
3. **å¤šæ¨¡æ€èåˆ**ï¼š4å±‚VLè‡ªæ³¨æ„åŠ›å……åˆ†èåˆè§†è§‰-è¯­è¨€ç‰¹å¾
4. **å¼ºå¤§çš„åŠ¨ä½œå»ºæ¨¡**ï¼š16å±‚DiTç¡®ä¿é«˜è´¨é‡åŠ¨ä½œç”Ÿæˆ
5. **å¤šå®ä½“æ”¯æŒ**ï¼šCategory-specific è®¾è®¡æ”¯æŒå¤šç§æœºå™¨äºº

è¿™ç§ç²¾å¿ƒè®¾è®¡çš„å±‚æ•°é…ç½®ä½¿å¾—GR00Tåœ¨ä¿æŒå¼ºå¤§åŠŸèƒ½çš„åŒæ—¶ï¼Œå…·æœ‰è‰¯å¥½çš„è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚
