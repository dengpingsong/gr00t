# ğŸ¤– GR00T æœºå™¨äººæ•°æ®é›†åˆ›å»ºå®Œæ•´æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•ä»é›¶å¼€å§‹åˆ›å»ºç¬¦åˆ GR00T LeRobot æ ¼å¼çš„æœºå™¨äººæ•°æ®é›†ï¼ŒåŒ…æ‹¬è®°å½•å…³èŠ‚ä¿¡æ¯ã€æ‘„åƒå¤´æ•°æ®å’Œæ ‡æ³¨ä¿¡æ¯çš„å®Œæ•´æµç¨‹ã€‚

## ğŸ“ 1. æ•°æ®é›†ç›®å½•ç»“æ„

é¦–å…ˆåˆ›å»ºæ ‡å‡†çš„ç›®å½•ç»“æ„ï¼š

```
my_robot_dataset/
â”œâ”€â”€ meta/                           # å…ƒæ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ episodes.jsonl             # ç‰‡æ®µä¿¡æ¯
â”‚   â”œâ”€â”€ tasks.jsonl                # ä»»åŠ¡ä¿¡æ¯  
â”‚   â”œâ”€â”€ modality.json              # æ¨¡æ€é…ç½® (GR00Tç‰¹æœ‰)
â”‚   â”œâ”€â”€ info.json                  # æ•°æ®é›†ä¿¡æ¯
â”‚   â””â”€â”€ stats.json                 # ç»Ÿè®¡ä¿¡æ¯ (å¯é€‰)
â”œâ”€â”€ data/                          # ç»“æ„åŒ–æ•°æ®
â”‚   â””â”€â”€ chunk-000/                 # æ•°æ®å—
â”‚       â”œâ”€â”€ episode_000000.parquet
â”‚       â”œâ”€â”€ episode_000001.parquet
â”‚       â””â”€â”€ ...
â””â”€â”€ videos/                        # è§†é¢‘æ•°æ®
    â””â”€â”€ chunk-000/
        â”œâ”€â”€ observation.images.front_camera/
        â”‚   â”œâ”€â”€ episode_000000.mp4
        â”‚   â””â”€â”€ episode_000001.mp4
        â””â”€â”€ observation.images.wrist_camera/
            â”œâ”€â”€ episode_000000.mp4
            â””â”€â”€ episode_000001.mp4
```

## ğŸ¥ 2. æ‘„åƒå¤´æ•°æ®è®°å½•

### 2.1 æ‘„åƒå¤´é…ç½®ç¤ºä¾‹

```python
import cv2
import numpy as np
from datetime import datetime
import threading
import queue

class CameraRecorder:
    def __init__(self, camera_configs):
        """
        camera_configs: {
            'front_camera': {'device_id': 0, 'resolution': (640, 480), 'fps': 30},
            'wrist_camera': {'device_id': 1, 'resolution': (640, 480), 'fps': 30},
        }
        """
        self.cameras = {}
        self.frame_queues = {}
        self.recording = False
        
        for name, config in camera_configs.items():
            cap = cv2.VideoCapture(config['device_id'])
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, config['resolution'][0])
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, config['resolution'][1])
            cap.set(cv2.CAP_PROP_FPS, config['fps'])
            
            self.cameras[name] = {
                'capture': cap,
                'config': config,
                'writer': None
            }
            self.frame_queues[name] = queue.Queue()
    
    def start_recording(self, episode_id, output_dir):
        """å¼€å§‹å½•åˆ¶æŒ‡å®šepisodeçš„è§†é¢‘"""
        self.recording = True
        
        # ä¸ºæ¯ä¸ªæ‘„åƒå¤´åˆ›å»ºVideoWriter
        for name, camera in self.cameras.items():
            config = camera['config']
            output_path = f"{output_dir}/observation.images.{name}/episode_{episode_id:06d}.mp4"
            
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(
                output_path, fourcc, config['fps'], config['resolution']
            )
            camera['writer'] = writer
        
        # å¯åŠ¨å½•åˆ¶çº¿ç¨‹
        self.record_threads = []
        for name in self.cameras.keys():
            thread = threading.Thread(target=self._record_camera, args=(name,))
            thread.start()
            self.record_threads.append(thread)
    
    def _record_camera(self, camera_name):
        """å½•åˆ¶å•ä¸ªæ‘„åƒå¤´çš„çº¿ç¨‹å‡½æ•°"""
        camera = self.cameras[camera_name]
        cap = camera['capture']
        writer = camera['writer']
        
        while self.recording:
            ret, frame = cap.read()
            if ret:
                writer.write(frame)
                # åŒæ—¶ä¿å­˜æ—¶é—´æˆ³ç”¨äºåŒæ­¥
                timestamp = datetime.now().timestamp()
                self.frame_queues[camera_name].put((timestamp, frame))
    
    def stop_recording(self):
        """åœæ­¢å½•åˆ¶"""
        self.recording = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        for thread in self.record_threads:
            thread.join()
        
        # é‡Šæ”¾VideoWriter
        for camera in self.cameras.values():
            if camera['writer']:
                camera['writer'].release()
                camera['writer'] = None
    
    def get_synchronized_frames(self):
        """è·å–åŒæ­¥çš„å¤šæ‘„åƒå¤´å¸§"""
        frames = {}
        timestamps = {}
        
        for name, frame_queue in self.frame_queues.items():
            if not frame_queue.empty():
                timestamp, frame = frame_queue.get()
                frames[name] = frame
                timestamps[name] = timestamp
        
        return frames, timestamps
```

### 2.2 ä½¿ç”¨ç¤ºä¾‹

```python
# é…ç½®æ‘„åƒå¤´
camera_configs = {
    'front_camera': {
        'device_id': 0,           # USBæ‘„åƒå¤´0
        'resolution': (640, 480),
        'fps': 30
    },
    'wrist_camera': {
        'device_id': 1,           # USBæ‘„åƒå¤´1  
        'resolution': (640, 480),
        'fps': 30
    }
}

# åˆ›å»ºå½•åˆ¶å™¨
recorder = CameraRecorder(camera_configs)

# å¼€å§‹å½•åˆ¶episode 0
recorder.start_recording(episode_id=0, output_dir="videos/chunk-000")

# ... æ‰§è¡Œæœºå™¨äººä»»åŠ¡ ...

# åœæ­¢å½•åˆ¶
recorder.stop_recording()
```

## ğŸ¦¾ 3. æœºå™¨äººå…³èŠ‚ä¿¡æ¯è®°å½•

### 3.1 å…³èŠ‚çŠ¶æ€è®°å½•å™¨

```python
import time
import pandas as pd
import numpy as np
from collections import defaultdict

class RobotStateRecorder:
    def __init__(self, robot_interface):
        """
        robot_interface: ä½ çš„æœºå™¨äººæ¥å£ç±»
        """
        self.robot = robot_interface
        self.episode_data = defaultdict(list)
        self.recording = False
        self.start_time = None
    
    def start_episode(self, episode_id):
        """å¼€å§‹æ–°çš„episodeå½•åˆ¶"""
        self.episode_id = episode_id
        self.episode_data = defaultdict(list)
        self.recording = True
        self.start_time = time.time()
        print(f"å¼€å§‹å½•åˆ¶ Episode {episode_id}")
    
    def record_step(self, action=None):
        """è®°å½•ä¸€ä¸ªæ—¶é—´æ­¥çš„æ•°æ®"""
        if not self.recording:
            return
        
        current_time = time.time()
        relative_time = current_time - self.start_time
        
        # 1. è®°å½•æ—¶é—´æˆ³
        self.episode_data['timestamp'].append(relative_time)
        
        # 2. è®°å½•å…³èŠ‚çŠ¶æ€ (è§‚æµ‹)
        joint_states = self.robot.get_joint_states()
        
        # ç¤ºä¾‹ï¼š7DOFæœºæ¢°è‡‚ + 2DOFå¤¹çˆª
        state_vector = np.concatenate([
            joint_states['left_arm_positions'],     # 7ä¸ªå…³èŠ‚ä½ç½®
            joint_states['left_arm_velocities'],    # 7ä¸ªå…³èŠ‚é€Ÿåº¦  
            joint_states['left_gripper_position'],  # 1ä¸ªå¤¹çˆªä½ç½®
            joint_states['base_position'],          # 3ä¸ªåŸºåº§ä½ç½®
            joint_states['base_orientation'],       # 4ä¸ªåŸºåº§å››å…ƒæ•°
        ])
        
        self.episode_data['observation.state'].append(state_vector)
        
        # 3. è®°å½•åŠ¨ä½œ (å¦‚æœæä¾›)
        if action is not None:
            self.episode_data['action'].append(action)
        
        # 4. è®°å½•ä»»åŠ¡æ ‡æ³¨ (å¯é€‰)
        task_description_index = 0  # å¯¹åº”tasks.jsonlä¸­çš„ç´¢å¼•
        self.episode_data['annotation.human.action.task_description'].append(task_description_index)
        
        # 5. è®°å½•æœ‰æ•ˆæ€§æ ‡æ³¨
        self.episode_data['annotation.human.validity'].append(1)  # 1=æœ‰æ•ˆ, 0=æ— æ•ˆ
        
    def end_episode(self, save_path):
        """ç»“æŸepisodeå¹¶ä¿å­˜æ•°æ®"""
        if not self.recording:
            return
        
        self.recording = False
        
        # è½¬æ¢ä¸ºDataFrame
        df_data = {}
        for key, values in self.episode_data.items():
            if key in ['observation.state', 'action']:
                # çŠ¶æ€å’ŒåŠ¨ä½œå­˜å‚¨ä¸ºæ•°ç»„åˆ—è¡¨
                df_data[key] = values
            else:
                df_data[key] = values
        
        df = pd.DataFrame(df_data)
        
        # ä¿å­˜ä¸ºparquetæ ¼å¼
        output_path = f"{save_path}/episode_{self.episode_id:06d}.parquet"
        df.to_parquet(output_path, index=False)
        
        print(f"Episode {self.episode_id} ä¿å­˜åˆ°: {output_path}")
        print(f"æ•°æ®é•¿åº¦: {len(df)} æ­¥")
        
        return len(df)  # è¿”å›episodeé•¿åº¦
```

### 3.2 æœºå™¨äººæ¥å£ç¤ºä¾‹

```python
class RobotInterface:
    """æœºå™¨äººæ¥å£ç¤ºä¾‹ - æ ¹æ®ä½ çš„å®é™…æœºå™¨äººä¿®æ”¹"""
    
    def __init__(self):
        # åˆå§‹åŒ–æœºå™¨äººè¿æ¥
        # self.robot = your_robot_driver()
        pass
    
    def get_joint_states(self):
        """è·å–å½“å‰å…³èŠ‚çŠ¶æ€"""
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„æœºå™¨äººAPIä¿®æ”¹
        
        # ç¤ºä¾‹ï¼š7DOFæœºæ¢°è‡‚
        joint_states = {
            'left_arm_positions': np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]),
            'left_arm_velocities': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
            'left_gripper_position': np.array([0.05]),  # å¤¹çˆªå¼€åˆåº¦
            'base_position': np.array([0.0, 0.0, 0.0]), # x, y, z
            'base_orientation': np.array([0.0, 0.0, 0.0, 1.0]), # quaternion
        }
        
        return joint_states
    
    def execute_action(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        # æ ¹æ®actionæ§åˆ¶æœºå™¨äºº
        # self.robot.set_joint_positions(action[:7])  # å…³èŠ‚ä½ç½®
        # self.robot.set_gripper(action[7])           # å¤¹çˆª
        pass
```

## ğŸ“ 4. å…ƒæ•°æ®æ–‡ä»¶åˆ›å»º

### 4.1 åˆ›å»º modality.json

```python
def create_modality_config(save_path):
    """åˆ›å»ºæ¨¡æ€é…ç½®æ–‡ä»¶"""
    
    modality_config = {
        "state": {
            "left_arm_positions": {"start": 0, "end": 7},
            "left_arm_velocities": {"start": 7, "end": 14}, 
            "left_gripper": {"start": 14, "end": 15},
            "base_position": {"start": 15, "end": 18},
            "base_orientation": {"start": 18, "end": 22}
        },
        "action": {
            "left_arm": {"start": 0, "end": 7},
            "left_gripper": {"start": 7, "end": 8},
            "base_motion": {"start": 8, "end": 11}
        },
        "video": {
            "front_camera": {
                "original_key": "observation.images.front_camera"
            },
            "wrist_camera": {
                "original_key": "observation.images.wrist_camera"
            }
        },
        "annotation": {
            "human.action.task_description": {},
            "human.validity": {}
        }
    }
    
    import json
    with open(f"{save_path}/modality.json", "w") as f:
        json.dump(modality_config, f, indent=2)
```

### 4.2 åˆ›å»ºå…¶ä»–å…ƒæ•°æ®

```python
def create_metadata_files(save_path, episodes_info, tasks_list):
    """åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶"""
    
    # 1. episodes.jsonl
    with open(f"{save_path}/episodes.jsonl", "w") as f:
        for ep_info in episodes_info:
            f.write(json.dumps(ep_info) + "\n")
    
    # 2. tasks.jsonl  
    with open(f"{save_path}/tasks.jsonl", "w") as f:
        for i, task in enumerate(tasks_list):
            f.write(json.dumps({"task_index": i, "task": task}) + "\n")
    
    # 3. info.json
    info = {
        "codebase_version": "1.0.0",
        "data_path": "data",
        "dataset_type": "LeRobotDataset", 
        "fps": 30,
        "robot_type": "my_robot",
        "total_episodes": len(episodes_info),
        "total_frames": sum(ep["length"] for ep in episodes_info),
        "video": True
    }
    
    with open(f"{save_path}/info.json", "w") as f:
        json.dump(info, f, indent=2)
```

## ğŸš€ 5. å®Œæ•´çš„æ•°æ®æ”¶é›†æµç¨‹

### 5.1 ä¸»è¦æ”¶é›†è„šæœ¬

```python
import os
import json
from pathlib import Path

def collect_robot_dataset(dataset_path, num_episodes=10):
    """å®Œæ•´çš„æ•°æ®æ”¶é›†æµç¨‹"""
    
    # 1. åˆ›å»ºç›®å½•ç»“æ„
    dataset_path = Path(dataset_path)
    (dataset_path / "meta").mkdir(parents=True, exist_ok=True)
    (dataset_path / "data" / "chunk-000").mkdir(parents=True, exist_ok=True)
    (dataset_path / "videos" / "chunk-000" / "observation.images.front_camera").mkdir(parents=True, exist_ok=True)
    (dataset_path / "videos" / "chunk-000" / "observation.images.wrist_camera").mkdir(parents=True, exist_ok=True)
    
    # 2. åˆå§‹åŒ–è®°å½•å™¨
    camera_configs = {
        'front_camera': {'device_id': 0, 'resolution': (640, 480), 'fps': 30},
        'wrist_camera': {'device_id': 1, 'resolution': (640, 480), 'fps': 30}
    }
    
    camera_recorder = CameraRecorder(camera_configs)
    robot_interface = RobotInterface()
    state_recorder = RobotStateRecorder(robot_interface)
    
    # 3. æ•°æ®æ”¶é›†
    episodes_info = []
    tasks_list = ["æŠ“å–çº¢è‰²æ–¹å—", "æœ‰æ•ˆæ•°æ®"]
    
    for episode_id in range(num_episodes):
        print(f"\n=== Episode {episode_id} ===")
        
        # å¼€å§‹å½•åˆ¶
        camera_recorder.start_recording(episode_id, dataset_path / "videos" / "chunk-000")
        state_recorder.start_episode(episode_id)
        
        # æ‰§è¡Œä»»åŠ¡ (è¿™é‡Œéœ€è¦ä½ çš„å…·ä½“ä»»åŠ¡é€»è¾‘)
        episode_length = execute_robot_task(state_recorder, robot_interface)
        
        # åœæ­¢å½•åˆ¶
        camera_recorder.stop_recording()
        actual_length = state_recorder.end_episode(dataset_path / "data" / "chunk-000")
        
        # è®°å½•episodeä¿¡æ¯
        episodes_info.append({
            "episode_index": episode_id,
            "tasks": ["æŠ“å–çº¢è‰²æ–¹å—", "æœ‰æ•ˆæ•°æ®"],
            "length": actual_length
        })
        
        print(f"Episode {episode_id} å®Œæˆï¼Œé•¿åº¦: {actual_length}")
    
    # 4. åˆ›å»ºå…ƒæ•°æ®
    create_modality_config(dataset_path / "meta")
    create_metadata_files(dataset_path / "meta", episodes_info, tasks_list)
    
    print(f"\næ•°æ®é›†åˆ›å»ºå®Œæˆ: {dataset_path}")
    print(f"æ€»episodeæ•°: {len(episodes_info)}")
    print(f"æ€»å¸§æ•°: {sum(ep['length'] for ep in episodes_info)}")

def execute_robot_task(state_recorder, robot_interface):
    """æ‰§è¡Œå…·ä½“çš„æœºå™¨äººä»»åŠ¡"""
    
    # è¿™é‡Œå®ç°ä½ çš„å…·ä½“ä»»åŠ¡é€»è¾‘
    # ä¾‹å¦‚ï¼šé¥æ§æ“ä½œã€ç¤ºæ•™å­¦ä¹ ã€è½¨è¿¹å›æ”¾ç­‰
    
    for step in range(100):  # å‡è®¾ä»»åŠ¡100æ­¥
        # 1. è·å–å½“å‰çŠ¶æ€å¹¶è®°å½•
        state_recorder.record_step()
        
        # 2. ç”Ÿæˆæˆ–è·å–åŠ¨ä½œ
        action = get_next_action()  # ä½ çš„åŠ¨ä½œç”Ÿæˆé€»è¾‘
        
        # 3. æ‰§è¡ŒåŠ¨ä½œ
        robot_interface.execute_action(action)
        
        # 4. å»¶æ—¶
        time.sleep(1/30)  # 30FPS
    
    return 100  # è¿”å›episodeé•¿åº¦

def get_next_action():
    """è·å–ä¸‹ä¸€ä¸ªåŠ¨ä½œ - æ ¹æ®ä½ çš„æ§åˆ¶æ–¹å¼å®ç°"""
    # ç¤ºä¾‹ï¼šéšæœºåŠ¨ä½œ
    return np.random.randn(8)  # 7ä¸ªå…³èŠ‚ + 1ä¸ªå¤¹çˆª

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    collect_robot_dataset("./my_robot_dataset", num_episodes=50)
```

## ğŸ® 6. ä¸åŒæ•°æ®æ”¶é›†æ–¹å¼

### 6.1 é¥æ§æ“ä½œæ”¶é›†

```python
import pygame

class TeleopController:
    def __init__(self):
        pygame.init()
        self.joystick = pygame.joystick.Joystick(0)
        self.joystick.init()
    
    def get_teleop_action(self):
        """è·å–é¥æ§è¾“å…¥"""
        pygame.event.pump()
        
        # è¯»å–æ‘‡æ†è¾“å…¥
        left_x = self.joystick.get_axis(0)
        left_y = self.joystick.get_axis(1)
        right_x = self.joystick.get_axis(2)
        right_y = self.joystick.get_axis(3)
        
        # è½¬æ¢ä¸ºæœºå™¨äººåŠ¨ä½œ
        action = np.array([
            left_x * 0.1,   # å…³èŠ‚1
            left_y * 0.1,   # å…³èŠ‚2
            right_x * 0.1,  # å…³èŠ‚3
            right_y * 0.1,  # å…³èŠ‚4
            # ... å…¶ä»–å…³èŠ‚
        ])
        
        return action
```

### 6.2 ç¤ºæ•™å­¦ä¹ æ”¶é›†

```python
class DemonstrationRecorder:
    def __init__(self, robot_interface):
        self.robot = robot_interface
    
    def record_demonstration(self, state_recorder):
        """è®°å½•äººç±»ç¤ºæ•™"""
        
        print("å¼€å§‹ç¤ºæ•™ï¼Œè¯·ç§»åŠ¨æœºå™¨äºº...")
        
        while True:
            # æ£€æµ‹æ˜¯å¦åœæ­¢
            if self.should_stop():
                break
            
            # è®°å½•å½“å‰çŠ¶æ€ä½œä¸ºç¤ºæ•™è½¨è¿¹
            state_recorder.record_step()
            
            time.sleep(1/30)
        
        print("ç¤ºæ•™ç»“æŸ")
    
    def should_stop(self):
        """æ£€æµ‹åœæ­¢æ¡ä»¶"""
        # ä¾‹å¦‚ï¼šæ£€æµ‹æŒ‰é”®ã€åŠ›ä¼ æ„Ÿå™¨ç­‰
        return False
```

## ğŸ“Š 7. æ•°æ®è´¨é‡æ£€æŸ¥

### 7.1 æ•°æ®éªŒè¯è„šæœ¬

```python
def validate_dataset(dataset_path):
    """éªŒè¯æ•°æ®é›†å®Œæ•´æ€§"""
    
    dataset_path = Path(dataset_path)
    
    # 1. æ£€æŸ¥ç›®å½•ç»“æ„
    required_dirs = [
        "meta", "data/chunk-000", 
        "videos/chunk-000"
    ]
    
    for dir_path in required_dirs:
        if not (dataset_path / dir_path).exists():
            print(f"âŒ ç¼ºå°‘ç›®å½•: {dir_path}")
            return False
    
    # 2. æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶
    required_files = [
        "meta/episodes.jsonl",
        "meta/tasks.jsonl", 
        "meta/modality.json",
        "meta/info.json"
    ]
    
    for file_path in required_files:
        if not (dataset_path / file_path).exists():
            print(f"âŒ ç¼ºå°‘æ–‡ä»¶: {file_path}")
            return False
    
    # 3. æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
    with open(dataset_path / "meta/episodes.jsonl") as f:
        episodes = [json.loads(line) for line in f]
    
    for ep in episodes:
        ep_id = ep["episode_index"]
        
        # æ£€æŸ¥parquetæ–‡ä»¶
        parquet_path = dataset_path / f"data/chunk-000/episode_{ep_id:06d}.parquet"
        if not parquet_path.exists():
            print(f"âŒ ç¼ºå°‘æ•°æ®æ–‡ä»¶: episode_{ep_id:06d}.parquet")
            return False
        
        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
        for camera in ["front_camera", "wrist_camera"]:
            video_path = dataset_path / f"videos/chunk-000/observation.images.{camera}/episode_{ep_id:06d}.mp4"
            if not video_path.exists():
                print(f"âŒ ç¼ºå°‘è§†é¢‘æ–‡ä»¶: {camera} episode_{ep_id:06d}.mp4")
                return False
    
    print("âœ… æ•°æ®é›†éªŒè¯é€šè¿‡!")
    return True

# ä½¿ç”¨ç¤ºä¾‹
validate_dataset("./my_robot_dataset")
```

## ğŸ¯ 8. æœ€ä½³å®è·µå»ºè®®

### 8.1 æ•°æ®æ”¶é›†å»ºè®®

1. **æ—¶é—´åŒæ­¥**: ç¡®ä¿è§†é¢‘å’Œå…³èŠ‚æ•°æ®çš„æ—¶é—´æˆ³å¯¹é½
2. **æ ‡å®šæ‘„åƒå¤´**: è®°å½•æ‘„åƒå¤´å†…å‚å’Œå¤–å‚ç”¨äºåç»­å¤„ç†
3. **å¤šæ ·åŒ–åœºæ™¯**: æ”¶é›†ä¸åŒå…‰ç…§ã€èƒŒæ™¯ã€ç‰©ä½“çš„æ•°æ®
4. **è´¨é‡æ§åˆ¶**: å®æ—¶ç›‘æ§æ•°æ®è´¨é‡ï¼ŒåŠæ—¶é‡å½•æ— æ•ˆæ•°æ®

### 8.2 å­˜å‚¨ä¼˜åŒ–

```python
# ä½¿ç”¨åˆé€‚çš„æ•°æ®ç±»å‹èŠ‚çœç©ºé—´
state_vector = np.array(joint_positions, dtype=np.float32)  # è€Œéfloat64

# è§†é¢‘å‹ç¼©è®¾ç½®
fourcc = cv2.VideoWriter_fourcc(*'h264')  # æ›´å¥½çš„å‹ç¼©ç‡
```

### 8.3 æ ‡æ³¨å»ºè®®

```python
# ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯
annotations = {
    "task_description": "æŠ“å–çº¢è‰²æ–¹å—å¹¶æ”¾å…¥è“è‰²å®¹å™¨",
    "task_phase": "approaching",  # approaching, grasping, lifting, placing
    "success": True,
    "difficulty": "easy",  # easy, medium, hard
    "environment": "lab_table_1"
}
```

é€šè¿‡è¿™ä¸ªå®Œæ•´çš„æŒ‡å—ï¼Œä½ å¯ä»¥åˆ›å»ºé«˜è´¨é‡çš„æœºå™¨äººæ•°æ®é›†ç”¨äºGR00Tæ¨¡å‹è®­ç»ƒã€‚è®°ä½æ ¹æ®ä½ çš„å…·ä½“æœºå™¨äººå¹³å°è°ƒæ•´æ¥å£å’Œé…ç½®ï¼
